{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAT4TG2Ivhq4"
      },
      "source": [
        "# L05\n",
        "# - Data Preprocessing and Machine Learning with Scikit-Learn\n",
        "# - KNN algorithm\n",
        "\n",
        "> Ref: Sebastian Raschka (sraschka@wisc.edu), 수정자 : Haechul Choi(HNU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05tsBYpvhq6"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIt8jycDvhq6"
      },
      "source": [
        "본 강의는 다음을 소개\n",
        "*   데이터 전처리에 매우 유용한 Python 라이브러리인 Pandas\n",
        "*   기계학습을 위해 잘 설계된 Scikit-learn 기계 학습 라이브러리\n",
        "*   *K*-Nearest Neighbors (KNN) 알고리즘"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google drive 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tui56yW8_OBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 작업할 폴더 위치 설정 (본인 폴더 경로에 맞게 수정필요)\n",
        "colab_path = \"\""
      ],
      "metadata": {
        "id": "lpPBaQf75HFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mlyifkjvhq7"
      },
      "source": [
        "## Pandas -- A Python Library for Working with Data Frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTSprx9yvhq7"
      },
      "source": [
        "- Pandas는 Python을 위한 가장 인기있고 편리한 데이터 처리 라이브러리 (공식 웹사이트: https://pandas.pydata.org)\n",
        "- NumPy Arrays와의 차이?\n",
        "    - 다른 자료형 허용 (열은 다른 data type을 가질 수 있다)\n",
        "    - 데이터 처리에 유용한 몇 가지 더 편리한 기능이 추가됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ct1220vhq7"
      },
      "source": [
        "### Loading Tabular Datasets from Text Files Using Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNDH1d3Lvhq8"
      },
      "source": [
        "- `read_csv` 명령어를 이용하여 CSV 파일을 `DataFrame` 클래스인 Pandas data frame object f로  로드\n",
        "- Data frame도 `head` 명령 지원; 처음 5개 열을 보여줌."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VATVm749vhq8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 실습자료 data 폴더의 iris.csv 파일 read\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gayLhyhHvhq9"
      },
      "outputs": [],
      "source": [
        "# df의 type 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaObSU3Mvhq9"
      },
      "source": [
        "- 차원(dimensions) 체크\n",
        "- `DataFrame` `shape` 속성은 NumPy array `shape`(강의 04)과 동일하게 동작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3TP0X6Zvhq9"
      },
      "outputs": [],
      "source": [
        "# df 의 shape 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0GE8wkGvhq-"
      },
      "source": [
        "#### Digression: Lambda Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXCp7V1vhq-"
      },
      "source": [
        "- \"lambda 함수\"는 기본적으로 \"일반 함수\"와 동일하지만 한 줄로 더 간결하게 작성될 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBO0jn69vhq-"
      },
      "outputs": [],
      "source": [
        "# some_func 예제 (일반 함수)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_2yheMuvhq_"
      },
      "outputs": [],
      "source": [
        "# some_func 예제 (lambda 함수)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVxAoiqOvhq-"
      },
      "source": [
        "### Basic Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vOjEAKNvhq-"
      },
      "source": [
        "- `apply` 메서드는 pandas `DataFrame` 항목을 **열** 축을 따라 조작하는 편리한 방법을 제공\n",
        "- 다음 코드는, 클래스 레이블을 문자열 표현 (예: \"Iris-Setosa\")에서 정수 표현 (예: 0)으로 변환하는 것. 이런 정수형 변환은 관례이며 다양한 기계 학습 도구와의 호환성을 위한 권장사항임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VetPrieMvhq-"
      },
      "outputs": [],
      "source": [
        "# df의 'Species' 열의 문자열 표현을 정수표현으로 변경\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVj34RCPvhq_"
      },
      "source": [
        "#### .map vs. .apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp7rbhDpvhq_"
      },
      "source": [
        "- 여러 개의 열 값을 매핑하려면, `apply`를 사용하는 것보다 'map' 메서드를 사용하는 것이 종종 더 편리\n",
        "- `apply` 메서드로 다음 코드를 달성하려면, `apply`를 세 번 호출해야 함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-atY2Xz4vhq_"
      },
      "outputs": [],
      "source": [
        "# .map 함수를 사용하여 Species 열의 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica' 값을 0, 1, 2로 한번에 mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pmCS2Nivhq_"
      },
      "source": [
        "-`tail` 메서드는 `head`와 유사하지만 기본적으로 마지막 다섯 행을 보여줌. 마지막 클래스 레이블 (Iris-Virginica)도 성공적으로 변환되었는지 확인하기 위해 이를 사용함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKvfNSd3vhq_"
      },
      "outputs": [],
      "source": [
        "# tail함수 사용 예제\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8A28wkyvhq_"
      },
      "source": [
        "- 실제로 `Species` 열의 모든 행 항목이 올바르게 변환되었는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IbDIlAbvhrA"
      },
      "outputs": [],
      "source": [
        "# numpy의 unique 함수를 사용하여 확인\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4Pn_SnvhrA"
      },
      "source": [
        "#### NumPy Arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzLT1SIavhrA"
      },
      "source": [
        "- Pandas의 데이터 프레임은 NumPy arrays을 기반으로 구축됨.\n",
        "- 많은 기계 학습 관련 도구들도 이제 pandas `DataFrame` 객체를 입력으로 지원하지만, 관례적으로 대부분의 작업에 NumPy arrays를 사용\n",
        "- `values` 속성을 통해 `DataFrame`의 기본 NumPy 배열에 접근 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLsyZUg9vhrA"
      },
      "outputs": [],
      "source": [
        "# value 속성을 사용하여 df의 'Species'열의 값들을 배열 형태로 y에 저장 및 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HScFEwpvhrA"
      },
      "source": [
        "- pandas `DataFrame`에서 열과 행을 접근하는 다양한 방법이 있다. 다음 문서를 참고, https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
        "-`iloc` 속성은 정수 기반의 인덱싱과 슬라이싱을 지원하며, 이는 NumPy arrays에서 인덱싱을 사용하는 방식과 유사하다(강의 04). 다음 표현식은 `DataFrame`에서 열 1, 2, 3, 4(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)을 선택한 다음 기본 NumPy arrays를 X에 할당한다.\n",
        "- 빠른 체크를 위해, NumPy arrays에서 처음 다섯 행만을 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUYbr9aNvhrA"
      },
      "outputs": [],
      "source": [
        "# df의 1, 2, 3, 4열의 값을 담은 X를 생성후 5번째 행까지 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqUvqjpRvhrB"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAZPVn9XvhrB"
      },
      "source": [
        "- MLxtend 라이브러리(http://rasbt.github.io/mlxtend/)는 \"머신러닝 확장\"을 의미하며, 머신러닝 및 데이터 과학 작업을 위한 몇몇 편리한 기능을 포함함.\n",
        "- 특히, scatterplotmatrix 함수로 데이터셋의 산점도 행렬을 표시할 수 있으며,  이는 데이터셋의 개요를 살펴보기에 유용함. (특성 간의 관계를 조사하거나 이상치를 찾는 등)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDxJdl3IvhrF"
      },
      "source": [
        "참고) 아래 코드에서 만약 mlxtend를 import 하지 못하면 mlxtend를 설치해야 함.\n",
        "> pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5-56ATSvhrF"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# 필요한 library import\n",
        "\n",
        "# df의 1, 2, 3, 4열의 값들로 다양한 형태의 figure 생성\n",
        "\n",
        "# figure 저장 및 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9puhtvvhrF"
      },
      "source": [
        "## Splitting a Dataset into Train, Validation, and Test Subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTiZnHcOvhrF"
      },
      "source": [
        "- 다음 코드 셀들은 데이터셋을 여러 하위 집합으로 분할하는 과정을 보여준다.\n",
        "- 데이터셋을 분할하기 전에 중요한 과정은 데이터셋을 섞는 것이다. (만약 데이터셋이 분할되기 전에 정렬되었다면 대표성이 없는 클래스 분포를 얻을 수 있기 때문에 데이터 셋을 섞는 과정이 필요하다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiBUpBR_vhrF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# dataset 을 랜덤하게 섞어 permuted_indices에 입력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f9GIumNvhrG"
      },
      "outputs": [],
      "source": [
        "# train/validation/test dataset의 갯수 지정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqV0aZj6vhrG"
      },
      "outputs": [],
      "source": [
        "# permute_indices의 인덱스 값들을 각각 train_ind, valid_ind, test_ind에 나눠 입력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SKsOPR3vhrG"
      },
      "outputs": [],
      "source": [
        "# 각 인덱스에 해당하는 실제 데이터 값들을 X로부터 X_triain, y_train/ X_valid, y_valid/ X_test, y_test로 입력\n",
        "\n",
        "\n",
        "\n",
        "# X_train의 shape 확인(지정한 train set의 수와 일치해야함)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6-CB15UvhrI"
      },
      "source": [
        "## K-Nearest Neighbors Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKJQBnfxvhrI"
      },
      "source": [
        "- 다음 코드 셀은 K-최근접 이웃 분류기의 매우 간단한 구현이다.\n",
        "- 이는 매우 느리고 비효율적인 구현이며, 실제 문제에서는 알고리즘을 처음부터 구현하는 대신 라이브러리(예: scikit-learn)를 사용하는 것이 권장된다.\n",
        "- 예를 들어, scikit-learn 라이브러리는 고급 데이터 구조(KD-Tree 및 Ball-Tree)를 사용하여 kNN을 훨씬 효과적이고 견고하게 구현하고 있다.  \n",
        "- 알고리즘을 처음부터 구현하는 것이 유용한 상황은 학습 및 교육 목적이거나, 새로운 알고리즘을 시도하고 싶을 때이다. 따라서 아래의 구현은 scikit-learn에서 어떻게 구현되는지를 개략적으로 소개한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktd2cIcsvhrI"
      },
      "outputs": [],
      "source": [
        "# KNNClassifier model 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdxAO29rvhrI"
      },
      "outputs": [],
      "source": [
        "# 생성한 knn_model의 예측 확인 (입력 X_valid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYoPtxwvhrI"
      },
      "source": [
        "위의 구현에서 _ 접미사를 가진 클래스 속성이 있음을 주목\n",
        "- 뒤에 오는 _ (예: `self.dataset_`)는 scikit-learn의 규칙이며 \"fit\" 속성임을 나타냅니다. 즉, fit 메서드를 호출한 후에만 사용할 수 있는 속성을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He3U24hevhrJ"
      },
      "source": [
        "## The Scikit-Learn Estimator API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE3wcvVevhrJ"
      },
      "source": [
        "- 아래는 분류 및 회귀 모델/알고리즘을 구현하기 위해 사용되는 scikit-learn 추정기 API의 개략적 구조이다.\n",
        "- 이전에 kNN 구현의 맥락에서 이러한 메서드를 보았지만, 아직 다루지 않은 흥미로운 추가 메서드인 `score`가 있다.\n",
        "- `score` 메서드는 내부적으로 특징 (`X`)에 대해 `predict`를 실행한 다음 예측된 타겟과 실제 타겟 `y`를 비교하여 성능을 계산한다.\n",
        "- 분류 모델의 경우, `score` 메서드는 분류 정확도 (범위 [0, 1])를 계산한다. - 즉, 올바르게 예측된 레이블의 비율. 회귀 모델의 경우, `score` 메서드는 결정 계수 ($R^2$)를 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeQw9HLPvhrJ"
      },
      "source": [
        "```python\n",
        "class SupervisedEstimator(...):\n",
        "\n",
        "    def __init__(self, hyperparam_1, ...):\n",
        "        self.hyperparm_1\n",
        "        ...\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        ...\n",
        "        self.fit_attribute_\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        ...\n",
        "        return y_pred\n",
        "\n",
        "    def score(self, X, y):\n",
        "        ...\n",
        "        return score\n",
        "    \n",
        "    def _private_method(self):\n",
        "        ...\n",
        "    ...\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3rAFAUDvhrJ"
      },
      "source": [
        "- 아래 그래픽은 scikit-learn이 분류 및 회귀 알고리즘/모델을 구현하기 위해 사용하는 `SupervisedEstimator` API의 사용법을 요약한 것이다.\n",
        "-  2D 데이터셋의 경우, 아래에 보여진 것처럼 mlxtend의 편리한 wrapper 함수를 사용하여 결정 영역을 그릴 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1sw8g5bvhrJ"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "\n",
        "\n",
        "# sklearn의 KNeighborsClassifier를 학습 및 결과를 표로 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU8ULhLvvhrK"
      },
      "source": [
        "## Stratification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b-_JTcYvhrK"
      },
      "source": [
        "- 이전에는 데이터셋을 섞고 학습, 검증 및 테스트 부분으로 나누기 위한 코드를 직접 작성했었는데, 이것에는 상당한 단점이 하나 있다.\n",
        "- 일반적으로 기계 학습 알고리즘/모델들이 학습, 검증 및 테스트 샘플이 동일한 분포에서 추출되었다고 가정한다. 하지만, 작은 데이터셋을 무작위로 나눈다면, 동일한 클래스 분포로 나뉘지 않을 수 있으므로 신뢰할 수 있는 모델 생성과 일반화 성능 추정이 어렵게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AY1hRKvhrK"
      },
      "source": [
        "- 데이터를 분할한 후 각 하위 집합에서 클래스 레이블 비율이 동일하도록 보장하는 방법은 일반적으로 \"계층화(stratification)\"라고 불리는 접근법을 사용한다.\n",
        "- 아래와 같이 `train_test_split` 메서드에서 클래스 레이블 배열을 `stratify` 매개변수에 전달하면 계층화가 지원된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MHBbsHxvhrK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# class 비율을 유지하며 분할\n",
        "\n",
        "# 분할한 데이터셋의 분포를 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFHJQllivhrK"
      },
      "outputs": [],
      "source": [
        "# train set에서 train/validation set 분할\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldb3xu_5vhrK"
      },
      "source": [
        "## Data Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRmZNYr-vhrK"
      },
      "source": [
        "- Iris 데이터셋의 경우, 모든 차원은 센티미터로 측정되었기 때문에 kNN의 맥락에서 \"스케일링\" 기능은 필요하지 않을 것이다 -- (제외하고자 하는 특성을 다르게 가중치를 부여하려는 경우는 제외)\n",
        "- 특성(feature) 스케일링 여부는 다루고 있는 문제에 따라 다르다.\n",
        "- 몇 가지 알고리즘들(특히 경사 하강법(L03에서 학습함) 등)은 데이터가 중앙에 위치하고 범위가 작을 경우 스케일링이 유용하다. 즉, 더 견고하고 수치적으로 안정적이며 빠르게 수렴한다.\n",
        "- 특성 스케일링을 위한 여러 가지 방법이 있지만, 여기서는 가장 일반적인 \"정규화\" 방법 중 두 가지만 다룬다 : 최소-최대 스케일링(Min-Max scaling)과 Z-점수 표준화(standardization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKXUCNlkvhrL"
      },
      "source": [
        "### Normalization -- Min-max scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfJPHSBxvhrL"
      },
      "source": [
        "- 최소-최대 스케일링은 특성을 [0, 1] 범위로 압축한다. 이는 단일 입력 $i$에 대한 다음과 같은 방정식을 통해 달성할 수 있다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaNZDTzTvhrL"
      },
      "source": [
        "$$x^{[i]}_{\\text{norm}} = \\frac{x^{[i]} - x_{\\text{min}} }{ x_{\\text{max}} - x_{\\text{min}} }$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQm7vA-wvhrL"
      },
      "source": [
        "- 다음은 NumPy를 사용하여 1D 입력 벡터(1개의 feature)에 대한 6개 데이터 인스턴스에 최소-최대 스케일링을 구현하고 적용하는 방법의 예제이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYkZ_6mYvhrL"
      },
      "outputs": [],
      "source": [
        "x = np.arange(6).astype(float)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbpH36fzvhrL"
      },
      "outputs": [],
      "source": [
        "# Scaling 및 결과 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuVzhe8uvhrL"
      },
      "source": [
        "### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhVl5N2QvhrL"
      },
      "source": [
        "- Z-점수 표준화는 특정 최적화(optimization) 방법(예: 경사 하강법)을 사용할 때 유용한 표준화 방법이다.\n",
        "- 특성을 표준화한 후, 해당 특성은 표준 정규 분포의 특성을 갖게 되며, 즉 단위 분산과 평균이 0 ($N(\\mu=0, \\sigma^2=1)$)인 분포를 갖는다. 그러나 이는 특성을 정규 분포를 따르지 않는 상태에서 정규 분포를 따르는 상태로 변환시키지는 않는다.\n",
        "- 특성을 표준화하기 위한 공식은 아래와 같으며, 단일 데이터 포인트 $x^{[i]}$에 대한 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WdRJAlJvhrM"
      },
      "source": [
        "$$x^{[i]}_{\\text{std}} = \\frac{x^{[i]} - \\mu_x }{ \\sigma_{x} }$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhshmpghvhrM"
      },
      "outputs": [],
      "source": [
        "x = np.arange(6).astype(float)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG31hliRvhrM"
      },
      "outputs": [],
      "source": [
        "# Standardization 및 결과 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825HPGo6vhrM"
      },
      "source": [
        "- 편리하게도, NumPy와 Pandas는 모두 `std` 메소드를 구현하고 있으며, 이 메소드는 표준 편차를 계산한다.\n",
        "- 아래에 표시된 결과의 차이를 주목하시요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGQXIccuvhrM"
      },
      "outputs": [],
      "source": [
        "# numpy와 pandas의 Standarization 결과 차이 확인\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQxQgVj_vhrM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEKuqPidvhrM"
      },
      "source": [
        "- 위의 결과가 다른 이유는 Pandas가 \"표본(sample)\" 표준 편차 ($s_x$)를 계산하는 반면 NumPy가 \"모집단(population)\" 표준 편차 ($\\sigma_x$)를 계산하기 때문이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDjLWEeKvhrM"
      },
      "source": [
        "$$s_x = \\sqrt{ \\frac{1}{n-1} \\sum^{n}_{i=1} (x^{[i]} - \\bar{x})^2 }$$\n",
        "\n",
        "$$\\sigma_x = \\sqrt{ \\frac{1}{n} \\sum^{n}_{i=1} (x^{[i]} - \\mu_x)^2 }$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxfNp8eivhrM"
      },
      "source": [
        "- 기계 학습은 일반적으로 대규모 데이터셋을 다루기 때문에 Bessel 보정(분모에서 자유도 하나를 빼는 것)에 대해 크게 신경 쓰지 않는다.\n",
        "- 더욱이, 여기서의 목표는 특정한 분포를 모델링하거나 분포 매개변수를 정확하게 추정하는 것이 아니다. 그러나 원한다면 NumPy의 'ddof' 매개변수를 사용하여 추가적인 자유도를 제거할 수도 있다. 그러나 실제로는 대부분의 경우 이것이 필요하지 않다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTkGNRR-vhrN"
      },
      "outputs": [],
      "source": [
        "# ddof 매개변수 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTwcJpe3vhrN"
      },
      "source": [
        "- 매우 중요한 개념은 추정된 정규화 매개변수를 사용하는 방법이다 (예: z-점수 정규화에서 평균과 표준 편차).\n",
        "- 특히, 훈련 세트에서 추정한 매개변수를 검증 및 테스트 세트를 변환하는 데 재사용하는 것이 중요하다. 이 매개변수를 다시 추정하는 것은 일반적인 \"초보자 실수\"이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8wiZzt4vhrN"
      },
      "outputs": [],
      "source": [
        "# 동일한 mean 값과 Sigma 값으로 Train/Validation/Test set Standarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp-7A9KMvhrN"
      },
      "source": [
        "### Scikit-Learn Transformer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEZlENmIvhrN"
      },
      "source": [
        "Scikit-Learn의 Transformer API는 **데이터의 전처리 및 변환**을 담당하는 중요한 기능을 제공하는 파트이다. 이 API를 사용하면 데이터를 처리하고 모델에 입력으로 사용하기 전에 데이터의 형태를 변환하거나 조작할 수 있다. 이러한 변환 작업은 데이터 품질 향상, 모델 성능 향상 및 기계 학습 파이프라인의 핵심 부분으로 중요하다.\n",
        "\n",
        "Transformer API의 주요 특징은 다음과 같다:\n",
        "\n",
        "- **Consistency (일관성)**: Scikit-Learn에서 제공되는 모든 변환기는 일관된 API를 가지고 있다. `fit`, `transform`, 그리고 `fit_transform` 메서드를 사용하여 데이터 변환을 수행할 수 있다.\n",
        "\n",
        "- **Modularity (모듈성)**: 각 변환 작업은 독립적인 변환기 객체로 구현된다. 이 객체들은 조합하여 복잡한 데이터 전처리 파이프라인을 구축할 수 있다.\n",
        "\n",
        "- **Composition (구성)**: 변환 작업은 파이프라인으로 구성될 수 있다. 파이프라인은 여러 변환기를 순차적으로 연결하여 데이터를 처리한다.\n",
        "\n",
        "- **Integration with Other Libraries (다른 라이브러리와의 통합)**: Scikit-Learn의 변환기는 다른 머신 러닝 라이브러리와 통합하기 쉽다. 예를 들어, 변환된 데이터를 다른 라이브러리의 모델에 바로 사용할 수 있다.\n",
        "\n",
        "- **Customization (사용자 정의)**: 필요한 경우 사용자 정의 변환기를 생성할 수 있다. 이를 통해 데이터를 특정 방식으로 변환하거나 사용자 지정 전처리 작업을 수행할 수 있다.\n",
        "\n",
        "Transformer API는 데이터 전처리 과정에서 다양한 작업을 수행할 수 있다. 예를 들어, 데이터 스케일링, 특성 추출, 텍스트 데이터의 토큰화, 범주형 데이터의 인코딩 등이 있다. 이러한 변환 작업을 통해 데이터는 기계 학습 모델에 적합한 형태로 변환되어 모델의 성능을 향상시킬 수 있다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srSc8ur4vhrN"
      },
      "source": [
        "- scikit-learn의 변환기(API)는 일반적으로 예측기(estimator) API와 매우 유사하며, 주된 차이점은 변환기가 일반적으로 \"비지도 학습\"이라는 것이며, 이는 클래스 레이블이나 타겟 값을 사용하지 않는다는 것을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W41Hs3u7vhrO"
      },
      "source": [
        "- scikit-learn에서의 변환기의 전형적인 예시로는 `MinMaxScaler`와 `StandardScaler`가 있으며, 이들은 이전에 설명한대로 최솟값-최댓값 스케일링과 z-점수 정규화를 수행하는 데 사용할 수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sVJL1K7vhrO"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "\n",
        "# sklearn.preprocessing을 활용하여 Standarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwrxzsSvhrO"
      },
      "source": [
        "## Categorical Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7R5hzZ9vhrO"
      },
      "source": [
        "- 기계 학습 알고리즘의 입력으로 데이터셋을 전처리할 때, 범주형 변수를 어떻게 다루는지 주의해야 한다.\n",
        "- 범주형 변수는 크게 두 가지 범주로 나뉜다: 명목형(norminal, 데이터의 순서가 의미 없음)과 순서형(ordinal, 데이터의 순서가 있음)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgT2AR8QvhrO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# categoricaldata.csv read\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNKUI-GietUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBkCv5pBvhrO"
      },
      "source": [
        "- 위의 예에서 `size`는 순서형 변수의 예이다. 즉, 만약 이 글자들이 티셔츠 사이즈를 나타낸다면 M < L < XXL과 같은 순서를 정하는 것이 합리적일 것이다.\n",
        "- 따라서, 순서형 변수에 증가하는 값에 할당할 수 있다. 그러나 범주 간의 범위와 차이는 우리의 도메인 지식과 판단에 따라 달라진다.\n",
        "- 기계 학습 알고리즘을 위해 순서형 변수를 수치적 계산을 위한 적절한 표현으로 변환하기 위해, **Pandas**의 `map` 메서드를 아래와 같이 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Jd7ZWNvhrO"
      },
      "outputs": [],
      "source": [
        "# map 함수를 사용하여 df의 size 값 mapping 'M' -> 2, 'L' -> 3, 'XXL' -> 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW0OP3c_vhrO"
      },
      "source": [
        "- 기계 학습 알고리즘은 클래스 레이블의 경우 순서를 가정하지 않는다.\n",
        "- 여기서는 `map` 메서드를 사용하는 대신 scikit-learn의 `LabelEncoder`를 사용하여 클래스 레이블을 정수로 변환할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jha15RVPvhrP"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "\n",
        "# labelEncoder를 사용하여 범주형 데이터를 정수형으로 인코딩\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYM4OaxCvhrP"
      },
      "source": [
        "- 명목 변수를 적절하게 나타내는 것은 조금 더 까다로운 작업이다.\n",
        "- 기계 학습 알고리즘은 일반적으로 변수가 정수 값을 가질 경우 순서를 가정하기 때문에, 이러한 가정을 하지 않도록 약간의 \"특별한 방법\"을 적용해야 한니다.\n",
        "- 이 \"특별한 방법\"은 \"원-핫(one-hot)\" 인코딩이라고도 불리며, 아래에서 색상 변수에 대한 예시를 보여주는 것처럼 명목 변수를 이진 형태로 변환한다. (다시 말해, 주황 < 빨강 < 파랑과 같은 순서가 보통은 의미가 없기 때문에 이렇게 하는 것이다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYCYU48vhrP"
      },
      "outputs": [],
      "source": [
        "# 범주형 데이터를 가지고 있는 열을 원-핫 인코딩(one-hot encoding) 형태로 변경\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTqxuiivhrP"
      },
      "source": [
        "- 위의 코드를 실행하면 \"색상\"에 대한 3개의 새 변수가 생성된다. 각 변수는 이진값을 가진다.\n",
        "- 그러나 지금은 약간의 중복이 있다 (예: `color_green` 및 `color_red`의 값을 알고 있다면 `color_blue`의 값을 자동으로 알 수 있다).\n",
        "- 다중공선성(collinearity)은 문제를 일으킬 수 있지만 (예: 선형 회귀의 폐쇄 형태의 맥락에서 역행렬이 존재하지 않을 수 있음), 대부분의 알고리즘은 다중공선성을 다룰 수 있기 때문에 기계 학습에서는 일반적으로 그렇게 크게 신경 쓰지 않는다. (예: 경사 하강 최적화를 통해 학습되는 정규화 패널티와 같은 제약을 회귀 모델에 추가).\n",
        "- 그러나 가능하다면 다중공선성을 제거하는 것은 항상 좋은 생각이며, 원-핫 인코딩된 변수 중 하나의 열을 삭제함으로써 편리하게 수행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCJ8alBXvhrP"
      },
      "outputs": [],
      "source": [
        "# 다중공선성을 제거하기 위해 drop_first 매개변수 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTNWYLauvhrP"
      },
      "source": [
        "## Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ZY1IF1vhrP"
      },
      "source": [
        "- 누락된 데이터를 처리하는 여러 가지 방법이 있다.\n",
        "- 가장 간단한 접근 방법은 전체 열 또는 행을 제거하는 것이다.\n",
        "- 다른 간단한 방법은 특성의 평균, 중앙값, 최빈값 등을 사용하여 누락된 값을 보정하는 것이다.\n",
        "- 규칙이나 최선의 방법은 없으며, 적절한 누락 데이터 보정 방법의 선택은 여러분의 판단과 도메인 지식에 따라 달라진다.\n",
        "- 아래에는 누락된 데이터 처리에 대한 몇 가지 예시가 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXofxD77vhrP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(colab_path, 'data/missingdata.csv'))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qb1khwNvhrQ"
      },
      "outputs": [],
      "source": [
        "# df의 각 열에서 NaN 값에 해당 하는 값의 갯수\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFaeI-u4vhrQ"
      },
      "outputs": [],
      "source": [
        "# dropna 함수 사용(axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2de9nYbKvhrQ"
      },
      "outputs": [],
      "source": [
        "# dropna 함수 사용(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0pYv0vNvhrQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SimpleImputer 사용하여 NaN 값 처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4lKcUvNvhrQ"
      },
      "source": [
        "## Feature Transformation, Extraction, and Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xcj_5CJvhrQ"
      },
      "source": [
        "이미 매우 간단한 특징 변환 사례, 즉 정규화, 즉 최소-최대 스케일링 및 표준화를 다루었다. 다른 경우도 많이 있지만, 특징 전처리의 광범위한 내용은 기계 학습 수업의 범위를 벗어난다. 그러나 이후 이 과목에서 인기 있는 특성 선택(순차적 특성 선택) 및 특성 추출(예: 주성분 분석) 기술 중 일부를 살펴볼 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hu-1R6-vhrQ"
      },
      "source": [
        "## Scikit-Learn Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-FqnCH1vhrQ"
      },
      "source": [
        "- scikit-learn 파이프라인은 매우 편리하고 강력한 개념이다. 이것은 scikit-learn을 다른 기계 학습 라이브러리와 구분 짓는 요소 중 하나이다.\n",
        "- 파이프라인은 본질적으로 우리에게 추정기를 적합화하는 동안 일련의 전처리 단계를 함께 정의하게 해준다.\n",
        "- 파이프라인은 자동으로 훈련 세트에서 특성 스케일링 매개변수를 추정하고 이를 새 데이터에 적용하는 것과 같은 함정을 자동으로 처리할 것이다 (우리가 이전에 z-점수 표준화에서 논의한 것)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN0_Q2t4vhrR"
      },
      "source": [
        "- 아래는 특성 스케일링 단계를 kNN 분류기와 결합하는 예시 파이프라인이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNharJAVvhrR"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# sklearn.pipeline의 make_pipeline함수로 전처리 및 KNN Classifier pipeline 생성 후, 구조 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bUZ1CCovhrR"
      },
      "outputs": [],
      "source": [
        "# pipeline 학습 및 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM-fYNJUvhrR"
      },
      "source": [
        "- 위에서 볼 수 있듯이, 파이프라인 자체는 scikit-learn의 추정기 API를 따른다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLn7dONQvhrR"
      },
      "source": [
        "( 참고: scikit-learn의 [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)는 임의의 호출 가능한 함수에서 변환기 클래스를 만들 수 있게 해준다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C06VZHXBvhrS"
      },
      "source": [
        "## Intro Model Selection -- Pipelines and Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hhEbMGvhrS"
      },
      "source": [
        "- 기계 학습 실무에서는 종종 기계 학습 알고리즘의 하이퍼파라미터를 조정하여 좋은 설정을 찾아야 한다.\n",
        "- 하이퍼파라미터를 조정하고 결과 모델을 비교하고 선택하는 과정은 \"모델 선택(model selection)\"이라고도 한니다 (알고리즘 선택과 대조적).\n",
        "- 이 과정에 대한 자세한 내용은 이 과목의 뒷부분에서 \"모델 선택\" 및 \"알고리즘 선택\"과 같은 주제를 더 자세히 다룰 것이다.\n",
        "- 현재는 모델 선택을 수행하는 가장 간단한 방법을 소개하고 있다: \"홀드아웃 메서드\"를 사용하는 방법\n",
        "- 홀드아웃 메서드에서는 데이터 집합을 3개 하위 집합으로 나눈다: 훈련 데이터, 검증 데이터 및 테스트 데이터.\n",
        "- 일반화 성능의 추정치를 편향시키지 않기 위해 테스트 데이터를 한 번만 사용하려고 하며, 이것이 모델 선택 (하이퍼파라미터 조정)을 위해 검증 데이터를 사용하는 이유이다.\n",
        "- 여기서 검증 데이터는 일반화 성능의 추정치 역할도 하지만, 모델 선택 과정에서 반복적으로 사용되기 때문에 테스트 데이터에 대한 최종 추정치보다 더 편향될 수 있다 (다중 가설 검정과 비슷한 원리이다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxZELUt8vhrS"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from mlxtend.evaluate import PredefinedHoldoutSplit\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# GridSearch 예제\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXCMz9e6vhrS"
      },
      "outputs": [],
      "source": [
        "# GridSearch 결과 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X47dGTKSvhrS"
      },
      "source": [
        "`kneighborsclassifier__p` : KNN에서 거리 측정 방법에 대한 하이퍼파라미터(hyperparameter)\n",
        "- p = 1: 맨해튼 거리 (Manhattan distance) : 두 데이터 포인트 사이의 수평 및 수직 거리의 합으로 계산된다.\n",
        "$$∣x_2−x_1∣+∣y_2−y_1∣$$\n",
        "<br />\n",
        "- p = 2: 유클리디안 거리 (Euclidean distance) : 두 데이터 포인트 사이의 직선 거리로 계산된다.\n",
        "$$\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTTK-R_vhrS"
      },
      "source": [
        "`PredefinedHoldoutSplit` : mlxtend 라이브러리에서 제공되는 교차 검증 전략 중 하나로, 이미 정의된 검증 데이터셋을 활용하여 교차 검증을 수행할 수 있도록 도와주는 클래스\n",
        "- 기본적인 교차 검증은 데이터를 여러 개의 폴드(fold)로 나누어 모델을 훈련하고 검증하는 과정을 반복한다. 이때 검증 데이터를 randomly 선택할 수 있다.\n",
        "- 그러나 PredefinedHoldoutSplit은 검증 데이터셋이 이미 정해져 있는 경우에 유용하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYwit3VhvhrT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2IJZRghvhrT"
      },
      "outputs": [],
      "source": [
        "# Grid Search 결과 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAJ8VylcvhrT"
      },
      "source": [
        "## Further Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15XcUlv9vhrT"
      },
      "source": [
        "- Scikit-learn documentation: http://scikit-learn.org/stable/documentation.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#실습 과제\n",
        "- data 폴더의 titanic.csv를 read 하여 KNN Classification 예측을 수행해 결과를 출력\n",
        "- titanic.csv dataset은 `Name`, `Sex`, `Ash`, `Age`, `SibSp`등의 11개의 값을 바탕으로 생존 확률(0, 1)을 예측해야 함.  \n",
        "(Dataset read 후, pandas의 `loc`, `value`등을 활용하여 훈련데이터와 정답값을 분리하는 과정 및 전처리 과정 필요)  \n",
        "(pandas의 `drop`함수 활용 권장)\n",
        "\n",
        "- 위의 실습 내용을 참고하여 **60% 이상** 성능 달성\n",
        "\n",
        "- 과제는 해당 셀 아래에 작성\n",
        "- 권장사항: 데이터셋 분석\n",
        "- [[titanic dataset 정보]](https://www.kaggle.com/competitions/titanic/data?select=train.csv)"
      ],
      "metadata": {
        "id": "z_30bny1D8LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library import\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from mlxtend.evaluate import PredefinedHoldoutSplit\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# dataset read\n",
        "titanic_csv = pd.read_csv(os.path.join(colab_path, 'data/titanic.csv'))\n",
        "\n",
        "# data preprocessing\n",
        "\n",
        "\n",
        "# train/test set 분할 (test_size, random_state 및 stratify 값 변경 금지)\n",
        "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y,\n",
        "                                                                test_size=0.2, shuffle=True,\n",
        "                                                                random_state=123, stratify=y)\n",
        "\n",
        "# train/validation set 분할 (test_size 및 random_state 값 변경 금지)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid,y_train_valid,\n",
        "                                        test_size=0.2, shuffle=True,\n",
        "                                        random_state=123, stratify=y_train_valid)\n",
        "\n",
        "#pipeline 생성\n",
        "\n",
        "# GridSearch를 수행할 parameter 선언\n",
        "\n",
        "# GridSearch Setting\n",
        "\n",
        "# GridSearch 수행\n"
      ],
      "metadata": {
        "id": "3HM1CCHdEeX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearch 결과 출력\n",
        "grid_titanic.cv_results_"
      ],
      "metadata": {
        "id": "ry609z3JEoaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_titanic.best_score_)\n",
        "print(grid_titanic.best_params_)"
      ],
      "metadata": {
        "id": "fC3ieeV8Eu1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# titanic.csv 파일에 대한 예측 결과 (60% 이상 달성)\n",
        "clf = grid_titanic.best_estimator_\n",
        "clf.fit(X_train_titanic, y_train_titanic)\n",
        "print('Test accuracy: %.2f%%' % (clf.score(X_test_titanic, y_test_titanic)*100))"
      ],
      "metadata": {
        "id": "tQKQYGt7Ewh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc-autonumbering": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}